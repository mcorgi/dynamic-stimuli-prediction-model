{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df980c75-a584-440e-9377-d07db35eedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73eb5c3-0136-44bf-86d7-212936336091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/notebooks/sensorium_2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3a83a4-5354-42f5-aae4-15a68e9d879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.25.91 requires docutils<0.17,>=0.10, but you have docutils 0.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.4.0 requires dill<0.3.6, but you have dill 0.3.7 which is incompatible.\n",
      "awscli 1.25.91 requires botocore==1.27.90, but you have botocore 1.31.17 which is incompatible.\n",
      "awscli 1.25.91 requires docutils<0.17,>=0.10, but you have docutils 0.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.4.0 requires dill<0.3.6, but you have dill 0.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.1.0 which is incompatible.\n",
      "torchaudio 0.12.1+cu116 requires torch==1.12.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gradio\n",
      "  Downloading gradio-3.47.1-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (5.4.1)\n",
      "Collecting uvicorn>=0.14.0\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio) (23.0)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from gradio) (1.9.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (1.5.0)\n",
      "Collecting altair<6.0,>=4.2.0\n",
      "  Downloading altair-5.1.2-py3-none-any.whl (516 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.2/516.2 kB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.6.1)\n",
      "Collecting gradio-client==0.6.0\n",
      "  Downloading gradio_client-0.6.0-py3-none-any.whl (298 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (9.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.17.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
      "Collecting websockets<12.0,>=10.0\n",
      "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests~=2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.28.2)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.9/dist-packages (from gradio) (5.10.2)\n",
      "Collecting semantic-version~=2.0\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.4.0)\n",
      "Collecting orjson~=3.0\n",
      "  Downloading orjson-3.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client==0.6.0->gradio) (2023.9.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.64.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources<7.0,>=1.3->gradio) (3.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests~=2.0->gradio) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests~=2.0->gradio) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests~=2.0->gradio) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests~=2.0->gradio) (1.26.14)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions~=4.0\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
      "Collecting httpcore<0.19.0,>=0.18.0\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (18.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.14.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=63bf44b25f94c4d585d29d05ab50e8c6f40f92c49d55086b669f99b4023127b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/3c/79/71/1b3a0bc0ca224fc8af5087101cf28adc0054ea6521c4b758ec\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, semantic-version, python-multipart, orjson, h11, anyio, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, altair, gradio-client, gradio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.6.2\n",
      "    Uninstalling anyio-3.6.2:\n",
      "      Successfully uninstalled anyio-3.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.1.0 which is incompatible.\n",
      "datasets 2.4.0 requires dill<0.3.6, but you have dill 0.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 altair-5.1.2 anyio-3.7.1 fastapi-0.103.2 ffmpy-0.3.1 gradio-3.47.1 gradio-client-0.6.0 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 orjson-3.9.9 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2 websockets-11.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/sinzlab/nnfabrik.git\n",
    "!pip install -q git+https://github.com/sinzlab/neuralpredictors\n",
    "\n",
    "!pip install -q deeplake\n",
    "!pip install -q transformers --upgrade\n",
    "!pip install -q  av\n",
    "!pip install -q decord\n",
    "\n",
    "#bits and bytes\n",
    "# Install latest bitsandbytes & transformers, accelerate from source\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# Other requirements for the demo\n",
    "!pip install gradio\n",
    "!pip install sentencepiece\n",
    "\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -i https://test.pypi.org/simple/ bitsandbytes\n",
    "\n",
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180c99b5-bd87-4306-87bb-53cfb2f6a3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.17.0-py2.py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.17.0 tenacity-8.2.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0094a41a-1456-4529-8cdd-2286db89bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from functools import partial\n",
    "\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "# transformers stuff\n",
    "\n",
    "import av\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from decord import VideoReader\n",
    "\n",
    "import time\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d528e5-0ef2-438d-afb4-8f42e1c23124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_yaml(file_path: str):\n",
    "    \"\"\"\n",
    "    Load YAML file into a dictionary. \n",
    "    Args: file_path (str): \n",
    "    Path to the YAML file to be loaded\n",
    "    \"\"\"    \n",
    "    file_path = Path(file_path)\n",
    "    with file_path.open('r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d0c1f0d-faaf-4469-9dbe-f58078068367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrogershijin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf881cdca214a5f9a7011c850e51171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669616130335876, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/teaching/sandra/wandb/run-20231016_095816-isgdljw7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rogershijin/notebooks-teaching_sandra/runs/isgdljw7\" target=\"_blank\">royal-shape-15</a></strong> to <a href=\"https://wandb.ai/rogershijin/notebooks-teaching_sandra\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "# config = load_yaml(\"configs/test.yaml\")\n",
    "# wandb_project = config.pop(\"project\")\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    # project=wandb_project,\n",
    "    # track hyperparameters and run metadata\n",
    "    # config= config\n",
    "       \n",
    ")\n",
    "# config = wandb.config \n",
    "# checkpoint_dir = Path(config.checkpoint_root)/config.name\n",
    "checkpoint_dir = Path('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cfa77d-2727-442d-999f-56b970982e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmice = [\\n    'dynamic29156-11-10-Video-8744edeac3b4d1ce16b680916b5267ce',\\n    'dynamic29228-2-10-Video-8744edeac3b4d1ce16b680916b5267ce',\\n    'dynamic29234-6-9-Video-8744edeac3b4d1ce16b680916b5267ce',\\n    'dynamic29513-3-5-Video-8744edeac3b4d1ce16b680916b5267ce',\\n    'dynamic29514-2-9-Video-8744edeac3b4d1ce16b680916b5267ce',\\n]\\n\\nmice = [\\n    'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20',\\n    'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20',\\n    'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20',\\n    'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20',\\n    'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20',\\n]\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "paths = [\n",
    "'/storage/sensorium/data/dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20/',\n",
    "'/storage/sensorium/data/dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20/',\n",
    "'/storage/sensorium/data/dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20/',\n",
    "'/storage/sensorium/data/dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20/',\n",
    "'/storage/sensorium/data/dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20/'\n",
    "]\n",
    "\n",
    "'''\n",
    "mice = [\n",
    "    'dynamic29156-11-10-Video-8744edeac3b4d1ce16b680916b5267ce',\n",
    "    'dynamic29228-2-10-Video-8744edeac3b4d1ce16b680916b5267ce',\n",
    "    'dynamic29234-6-9-Video-8744edeac3b4d1ce16b680916b5267ce',\n",
    "    'dynamic29513-3-5-Video-8744edeac3b4d1ce16b680916b5267ce',\n",
    "    'dynamic29514-2-9-Video-8744edeac3b4d1ce16b680916b5267ce',\n",
    "]\n",
    "\n",
    "mice = [\n",
    "    'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20',\n",
    "    'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20',\n",
    "    'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20',\n",
    "    'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20',\n",
    "    'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20',\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f793dd31-9772-4556-99ea-1651e531a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2552612-fe9d-4a59-835b-89d649c69502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datajoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5961c220-cd96-46ca-9345-b8a4eeaece61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if imports of sensorium do not work \n",
    "# !ln -s ../sensorium\n",
    "\n",
    "import sys\n",
    "sys.path.append('/notebooks/teaching/sandra/sensorium_2023')\n",
    "sys.path.append('/notebooks/sensorium_2023')\n",
    "\n",
    "import torch\n",
    "from nnfabrik.utility.nn_helpers import set_random_seed\n",
    "set_random_seed(seed)\n",
    "\n",
    "from sensorium.datasets.mouse_video_loaders import mouse_video_loader\n",
    "from sensorium.utility.scores import get_correlations\n",
    "from nnfabrik.builder import get_trainer\n",
    "from sensorium.models.make_model import make_video_model\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5de4c135-e2e3-43b7-9319-d5e5752e19a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data..\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset = deeplake.load(f'/storage/sensorium/data/{mice[0]}/data')\n",
    "\n",
    "# dataset = deeplake.load(f'hub://sinzlab/Sensorium_2023_{mice[0]}_train')\n",
    "\n",
    "print(\"Loading data..\")\n",
    "data_loaders = mouse_video_loader(\n",
    "    paths=paths,\n",
    "    batch_size=1,\n",
    "    scale=1,\n",
    "    max_frame=None,\n",
    "    frames=64, # frames has to be > 50. If it fits on your gpu, we recommend 150\n",
    "    offset=-1,\n",
    "    include_behavior=True,\n",
    "    include_pupil_centers=True,\n",
    "    cuda=device!='cpu',\n",
    "    to_cut=False,\n",
    ")\n",
    "print('Data loaded')\n",
    "\n",
    "\n",
    "# if DEV:\n",
    "#     dataset = dataset[:10]\n",
    "# dataset.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e81f0e9b-2e20-4c8e-bbaf-1aef9ac9b546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['live_test_bonus', 'train', 'live_test_main', 'final_test_bonus', 'final_test_main', 'oracle'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a599f6c-9112-480a-b3cc-63bc16f5a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "MICE = sorted(list(data_loaders['train'].keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f8d01c4-79f1-457e-952d-afdc654d9d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20': {'live_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142aa880>,\n",
       "  'train': <torch.utils.data.dataloader.DataLoader at 0x7f45142aad30>,\n",
       "  'live_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142aad60>,\n",
       "  'final_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142aa340>,\n",
       "  'final_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142aa4c0>,\n",
       "  'oracle': <torch.utils.data.dataloader.DataLoader at 0x7f45142aa7c0>},\n",
       " 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20': {'live_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f4514432670>,\n",
       "  'train': <torch.utils.data.dataloader.DataLoader at 0x7f45143144f0>,\n",
       "  'live_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f4514314cd0>,\n",
       "  'final_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f4514314340>,\n",
       "  'final_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f4514314e80>,\n",
       "  'oracle': <torch.utils.data.dataloader.DataLoader at 0x7f45142bb190>},\n",
       " 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20': {'live_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142bb4f0>,\n",
       "  'train': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6ee0>,\n",
       "  'live_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6cd0>,\n",
       "  'final_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6c40>,\n",
       "  'final_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6a90>,\n",
       "  'oracle': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6610>},\n",
       " 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20': {'live_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142b6c70>,\n",
       "  'train': <torch.utils.data.dataloader.DataLoader at 0x7f451429ed00>,\n",
       "  'live_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f451429e0a0>,\n",
       "  'final_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f451429e640>,\n",
       "  'final_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142af610>,\n",
       "  'oracle': <torch.utils.data.dataloader.DataLoader at 0x7f45142af190>},\n",
       " 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20': {'live_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142afa60>,\n",
       "  'train': <torch.utils.data.dataloader.DataLoader at 0x7f45142afb80>,\n",
       "  'live_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142af9a0>,\n",
       "  'final_test_bonus': <torch.utils.data.dataloader.DataLoader at 0x7f45142af430>,\n",
       "  'final_test_main': <torch.utils.data.dataloader.DataLoader at 0x7f45142a67c0>,\n",
       "  'oracle': <torch.utils.data.dataloader.DataLoader at 0x7f45142a69d0>}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders_transpose = {}\n",
    "\n",
    "for stage, stage_dataloaders in data_loaders.items():\n",
    "    for mouse, dataloader in stage_dataloaders.items():\n",
    "        if mouse not in data_loaders_transpose:\n",
    "            data_loaders_transpose[mouse] = {stage: dataloader}\n",
    "        else:\n",
    "            data_loaders_transpose[mouse][stage] = dataloader\n",
    "\n",
    "MOUSE_SIZES = {}\n",
    "for mouse in MICE:\n",
    "    batch = next(iter(data_loaders_transpose[mouse]['train']))\n",
    "    MOUSE_SIZES[mouse] = batch.responses.shape[1]\n",
    "MOUSE_SIZES\n",
    "\n",
    "data_loaders_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0b5c2-5639-407d-abe4-013a03c590b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5aca828-24bf-4810-a242-689f9a5dcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d872aadd-8bc0-4bf5-ade2-1bbf90050bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import Linear4bit\n",
    "from torch.nn import Linear\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "        \n",
    "\n",
    "class Swappable(nn.Module):\n",
    "    \n",
    "    def __init__(self, named_layers: dict[str, nn.Module]):\n",
    "        super().__init__()\n",
    "        self.named_layers = named_layers\n",
    "        self._named_layers = nn.ModuleList(named_layers.values())\n",
    "    \n",
    "    def forward(self, x, name):\n",
    "        return self.named_layers[name](x)\n",
    "\n",
    "def pad_to_nearest_multiple(tensor, target_multiple):\n",
    "    \"\"\"Pads a tensor to the nearest multiple along the second dimension.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor to pad.\n",
    "        target_multiple (int): The target multiple to pad the second dimension to.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Padded tensor.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, hidden_size = tensor.size()\n",
    "    \n",
    "    # Calculate the padding size needed to reach the nearest multiple\n",
    "    padding_size = (target_multiple - (seq_len % target_multiple)) % target_multiple\n",
    "    \n",
    "    # Calculate the padding for the left and right sides\n",
    "    left_padding = padding_size // 2\n",
    "    right_padding = padding_size - left_padding\n",
    "    \n",
    "    # Create padding tuple\n",
    "    padding_tuple = (0, 0, left_padding, right_padding)\n",
    "    \n",
    "    # Apply padding\n",
    "    padded_tensor = torch.nn.functional.pad(tensor, padding_tuple, 'constant', 0)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "class Reducer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_seq_len=3137, target_seq_len=32, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.target_seq_len = target_seq_len\n",
    "        padded_len = input_seq_len if input_seq_len % target_seq_len  == 0 else (input_seq_len // target_seq_len + 1) * 32\n",
    "        stride = padded_len // target_seq_len\n",
    "        kernel_size = stride\n",
    "        self.conv1d = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride=stride)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x has size batch_size x seq_len x hidden_dim\n",
    "        '''\n",
    "        x = pad_to_nearest_multiple(x, self.target_seq_len)\n",
    "        print('x', x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class ReducerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_dim, input_seq_len=3137, target_seq_len=32, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.reducer = Reducer(input_seq_len=input_seq_len, target_seq_len=target_seq_len, hidden_dim=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.reducer(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SensoriumVivitForVideoClassification(VivitForVideoClassification):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # self.reducer = Re\n",
    "        # self.reducer = Reducer()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values=None,\n",
    "        mouse=None,\n",
    "        inputs_embeds=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vivit(\n",
    "            pixel_values,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "#         sequence_output = outputs[0]\n",
    "\n",
    "#         logits = self.classifier(sequence_output[:, 0, :])\n",
    "\n",
    "#         reduced = self.reducer(outputs.last_hidden_state)\n",
    "#         print('reduced', reduced)\n",
    "\n",
    "#         logits = self.classifier(reduced, mouse)\n",
    "#         print('logits', logits)\n",
    "#         reduced = self.reducer(outputs.last_hidden_state)\n",
    "#         print('reduced', reduced)\n",
    "\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :], mouse)\n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6b275b4-5f5e-478e-8f1d-8731ff1dbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 32\n",
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "model = SensoriumVivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\", quantization_config=bnb_config)\n",
    "\n",
    "# model.classifier= Linear(768, 38000, compute_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "\n",
    "# model = VivitWrapper(model)\n",
    "\n",
    "model.classifier = Swappable({mouse: Linear(768, output_size*WINDOW_SIZE) for mouse, output_size in MOUSE_SIZES.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "989ee943-5c80-457f-8fc7-d69954e05214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensoriumVivitForVideoClassification(\n",
       "  (vivit): VivitModel(\n",
       "    (embeddings): VivitEmbeddings(\n",
       "      (patch_embeddings): VivitTubeletEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): VivitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VivitLayer(\n",
       "          (attention): VivitAttention(\n",
       "            (attention): VivitSelfAttention(\n",
       "              (query): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VivitSelfOutput(\n",
       "              (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VivitIntermediate(\n",
       "            (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): FastGELUActivation()\n",
       "          )\n",
       "          (output): VivitOutput(\n",
       "            (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Swappable(\n",
       "    (_named_layers): ModuleList(\n",
       "      (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "      (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "      (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "      (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "      (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "622a30e7-ac94-4fc3-a5de-3b9b4f973377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "607e7fe1-95b5-4350-a43e-579e8234e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# black magic to make sure we train the classifiers\n",
    "# def make_module_require_grad(module, input, output):\n",
    "#     output.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a755b11-204d-4b72-ba80-fac4f2b27537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1031335744 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47f7a003-5ba0-4ce3-b486-a097c16e5c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensoriumVivitForVideoClassification(\n",
       "  (vivit): VivitModel(\n",
       "    (embeddings): VivitEmbeddings(\n",
       "      (patch_embeddings): VivitTubeletEmbeddings(\n",
       "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): VivitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x VivitLayer(\n",
       "          (attention): VivitAttention(\n",
       "            (attention): VivitSelfAttention(\n",
       "              (query): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): VivitSelfOutput(\n",
       "              (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): VivitIntermediate(\n",
       "            (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_act_fn): FastGELUActivation()\n",
       "          )\n",
       "          (output): VivitOutput(\n",
       "            (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Swappable(\n",
       "    (_named_layers): ModuleList(\n",
       "      (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "      (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "      (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "      (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "      (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "461237cd-aea7-4d33-975a-e8c2b08c440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}, Dtype: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95725e8b-bfd2-4e48-83cc-76cd3c547c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1970755712 || all params: 2016934784 || trainable%: 97.71043306078458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): SensoriumVivitForVideoClassification(\n",
       "      (vivit): VivitModel(\n",
       "        (embeddings): VivitEmbeddings(\n",
       "          (patch_embeddings): VivitTubeletEmbeddings(\n",
       "            (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): VivitEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x VivitLayer(\n",
       "              (attention): VivitAttention(\n",
       "                (attention): VivitSelfAttention(\n",
       "                  (query): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (key): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (value): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): VivitSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): VivitIntermediate(\n",
       "                (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (intermediate_act_fn): FastGELUActivation()\n",
       "              )\n",
       "              (output): VivitOutput(\n",
       "                (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Swappable(\n",
       "          (_named_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "            (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Swappable(\n",
       "            (_named_layers): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "              (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "              (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "# train the classifier layer\n",
    "for param in model.classifier.parameters():\n",
    "    # classifier.register_forward_hook(make_module_require_grad)\n",
    "    param.requires_grad_(True)\n",
    "# for param in model.reducer.parameters():\n",
    "#     # classifier.register_forward_hook(make_module_require_grad)\n",
    "#     param.requires_grad_(True)\n",
    "print_trainable_parameters(model)\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c310d977-c74a-45ab-9169-cdf166470673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): SensoriumVivitForVideoClassification(\n",
       "      (vivit): VivitModel(\n",
       "        (embeddings): VivitEmbeddings(\n",
       "          (patch_embeddings): VivitTubeletEmbeddings(\n",
       "            (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): VivitEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x VivitLayer(\n",
       "              (attention): VivitAttention(\n",
       "                (attention): VivitSelfAttention(\n",
       "                  (query): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (key): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (value): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): VivitSelfOutput(\n",
       "                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): VivitIntermediate(\n",
       "                (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (intermediate_act_fn): FastGELUActivation()\n",
       "              )\n",
       "              (output): VivitOutput(\n",
       "                (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Swappable(\n",
       "          (_named_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "            (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "            (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Swappable(\n",
       "            (_named_layers): ModuleList(\n",
       "              (0): Linear(in_features=768, out_features=251616, bias=True)\n",
       "              (1): Linear(in_features=768, out_features=253056, bias=True)\n",
       "              (2): Linear(in_features=768, out_features=262464, bias=True)\n",
       "              (3): Linear(in_features=768, out_features=254048, bias=True)\n",
       "              (4): Linear(in_features=768, out_features=259904, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67ac9790-6c34-441b-89e7-958b5150d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we're training the classifiers\n",
    "assert next(model.classifier.parameters()).requires_grad\n",
    "assert next(model.classifier.parameters()).device == model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41c6e2e5-edfc-4d2d-8ca6-8d4ee9b471cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = bitsandbytes.optim.AdamW(model.parameters(),lr=3e-5,optim_bits=8, is_paged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe903186-4fa3-4ada-9d84-e829ab9e9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a03efeb7-3da6-40ef-8139-c0fe09cc814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa67f675-ed7d-4eaf-97f8-0b7e47de55d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8202 ('live_test_bonus', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20')\n",
      "[(8202, 'live_test_bonus', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8202, 'train', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8202, 'live_test_main', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8202, 'final_test_bonus', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8202, 'final_test_main', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8202, 'oracle', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'live_test_bonus', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'train', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'live_test_main', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'final_test_bonus', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'final_test_main', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'oracle', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'live_test_bonus', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'train', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'live_test_main', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'final_test_bonus', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'final_test_main', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'oracle', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'live_test_bonus', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'train', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'live_test_main', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'final_test_bonus', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'final_test_main', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'oracle', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'live_test_bonus', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'train', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'live_test_main', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'final_test_bonus', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'final_test_main', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'oracle', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20')]\n",
      "[(8202, 'train', 'dynamic29647-19-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (8122, 'train', 'dynamic29755-2-8-Video-9b4f6a1a067fe51e15306b9628efea20'), (7939, 'train', 'dynamic29712-5-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7908, 'train', 'dynamic29623-4-9-Video-9b4f6a1a067fe51e15306b9628efea20'), (7863, 'train', 'dynamic29515-10-12-Video-9b4f6a1a067fe51e15306b9628efea20')]\n"
     ]
    }
   ],
   "source": [
    "# find longest response mouse\n",
    "max_length = 0\n",
    "max_mouse = None\n",
    "mice_lengths = []\n",
    "mice_lengths_train = []\n",
    "for stage, stage_dataloaders in data_loaders.items():\n",
    "    for mouse, dataloader in stage_dataloaders.items():\n",
    "        for batch in dataloader:\n",
    "            length = batch.responses.shape[1]\n",
    "            mice_lengths.append((length, stage, mouse))\n",
    "            if 'train' in stage:\n",
    "                mice_lengths_train.append((length, stage, mouse))\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "                max_mouse = (stage, mouse)\n",
    "            break\n",
    "print(max_length, max_mouse)\n",
    "print(sorted(mice_lengths, key=lambda elt: elt[0], reverse=True))\n",
    "print(sorted(mice_lengths_train, key=lambda elt: elt[0], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1e105f3-bdd8-4c73-a663-fb95403f3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dataloaders(dataloaders, verbose=True):\n",
    "    iterators = {mouse: iter(loader) for mouse,loader in dataloaders.items()}\n",
    "    total = sum([len(loader) for loader in dataloaders.values()])\n",
    "    if verbose:\n",
    "        print('total from concatenate loaders', total)\n",
    "    count = 0\n",
    "    while True:\n",
    "        for mouse, iterator in iterators.items():\n",
    "            if count > total:\n",
    "                return\n",
    "            try:\n",
    "                yield mouse, next(iterator)\n",
    "                count += 1\n",
    "            except StopIteration:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19903e89-0f8e-4ca9-b607-3309f2b0ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd42df-ff9e-4c38-af03-3c4845f3dd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches 1744\n",
      "EPOCH 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5594dbdf5c724813af127c37a5e2a8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total from concatenate loaders 1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "best_val_loss = math.inf\n",
    "save_period = 1\n",
    "val_period = 1\n",
    "n_epochs = 3\n",
    "checkpoint_dir = Path(\"checkpoints\")/str(datetime.now()).replace(\" \", \"_\")\n",
    "checkpoint_dir.mkdir(parents = True)\n",
    "train_total = sum([len(loader) for loader in data_loaders['train'].values()])\n",
    "print('total batches', train_total)\n",
    "steps = 0\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    print('EPOCH', epoch)\n",
    "    model.train()\n",
    "    for mouse, batch in tqdm(concatenate_dataloaders(data_loaders['train']), total=train_total):\n",
    "        # model.classifier.to('cpu')\n",
    "        # model.classifier = classifiers[mouse].to('cuda:0')\n",
    "        # for batch in tqdm(data_loaders['train'][mouse]):\n",
    "            # print(batch.videos)\n",
    "        start = random.randint(50, 150-64)\n",
    "        videos = batch.videos[:,:,start:start+64:2,:,:].permute(0,2,3,4,1)\n",
    "        video_range = videos.max()-videos.min()\n",
    "        videos -= videos.max() - video_range/2\n",
    "        videos /= video_range + 0.01\n",
    "        videos += .5\n",
    "        videos = videos.squeeze()\n",
    "        videos = image_processor(list(videos), return_tensors=\"pt\")\n",
    "        videos['pixel_values'] = videos['pixel_values'].cuda()\n",
    "        responses = batch.responses[:,:,start:start+64:2].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(**videos, mouse=mouse)\n",
    "        # print(output.logits)\n",
    "        train_loss= loss_fn(output.logits.reshape(-1, MOUSE_SIZES[mouse], WINDOW_SIZE), torch.log(torch.nn.functional.relu(responses)+1)) #torch.log(batch['responses'])\n",
    "        # print(train_loss)\n",
    "        wandb.log({'train_loss':train_loss.item()})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        if DEBUG:\n",
    "            break\n",
    "        if steps % 500 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_dir/f\"step_{steps}.pt\")\n",
    "            \n",
    "    \n",
    "    if steps % save_period == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, checkpoint_dir/f\"{epoch}.pt\")\n",
    "\n",
    "    if epoch%val_period==0:\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for mouse in data_loaders['oracle']:\n",
    "            # model.classifier.to('cpu')\n",
    "            # model.classifier = classifiers[mouse].to('cuda:0')\n",
    "            model.eval()\n",
    "            for batch in data_loaders['oracle'][mouse]:\n",
    "                start = random.randint(50, 150-64)\n",
    "                videos = batch.videos[:,:,start:start+64:2,:,:].permute(0,2,3,4,1)\n",
    "                video_range = videos.max()-videos.min()\n",
    "                videos -= videos.max() - video_range/2\n",
    "                videos /= video_range + 0.01\n",
    "                videos += .5\n",
    "                videos = videos.squeeze()\n",
    "                videos = image_processor(list(videos), return_tensors=\"pt\")\n",
    "                videos['pixel_values'] = videos['pixel_values'].cuda()\n",
    "                responses = batch.responses[:,:,start:start+64:2].cuda()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(**videos, mouse=mouse)\n",
    "                    val_loss= loss_fn(output.logits.reshape(-1, MOUSE_SIZES[mouse], WINDOW_SIZE), torch.log(torch.nn.functional.relu(responses)+1)) #torch.log(batch['responses'])          \n",
    "                total_val_loss+=val_loss\n",
    "                if DEBUG:\n",
    "                    break\n",
    "            wandb.log({'val_loss':total_val_loss.item()})\n",
    "            print(\"train_loss\", train_loss.item())\n",
    "            print(\"val loss:\", total_val_loss.item())\n",
    "            if total_val_loss < best_val_loss:\n",
    "                best_val_loss = total_val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': total_val_loss,\n",
    "                }, checkpoint_dir/\"best.pt\")\n",
    "                print(f\"save checkpoint to {checkpoint_dir/'best.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f6837e8-b2c2-408a-b557-0d78bcec6706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac000f92-4127-4b36-8cb7-cdb774d9be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, checkpoint_dir/f\"step_{steps}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8802d0bd-b51d-4f7c-92fc-c1974c9dab4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('checkpoints/2023-10-16_09:14:41.068286')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bccfa90-5791-42a3-a9fc-113178744f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r {str(checkpoint_dir)} /storage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac84f1-4192-4d5c-b32f-c51dc85b5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes.nn import Linear4bit\n",
    "from torch.nn import Linear\n",
    "from transformers import VivitImageProcessor, VivitModel, VivitForVideoClassification\n",
    "\n",
    "checkpoint_dir = None\n",
    "checkpoint = torch.load(checkpoint_dir/'best.pt')\n",
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\", quantization_config=bnb_config)\n",
    "# model.classifier= Linear(768, 38000, compute_dtype=torch.bfloat16).cuda()\n",
    "model.classifier= Linear(768, 7495*WINDOW_SIZE).cuda()\n",
    "# model = VivitWrapper(model)\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "inputs['pixel_values'] = inputs['pixel_values'].repeat(2,1,1,1,1)\n",
    "print(inputs['pixel_values'].shape)\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec376c8-a6e6-4903-8164-1f4585438c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_test_loss = 0\n",
    "predictions = []\n",
    "for batch in dataloaders['oracle']:\n",
    "    with torch.no_grad():\n",
    "        output = model(**batch['videos'])\n",
    "        predictions.append(output.logits.reshape(-1,7495,WINDOW_SIZE).argmax(dim=1))\n",
    "        test_loss= loss_fn(output.logits.reshape(-1,7495,WINDOW_SIZE), torch.log(torch.nn.functional.relu(batch['responses'])+1)) #torch.log(batch['responses'])          \n",
    "    total_test_loss+=test_loss\n",
    "wandb.log({'test_loss':total_test_loss})\n",
    "print(\"test loss\")\n",
    "print(total_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6090432-35e9-4028-9765-c1caba256fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
